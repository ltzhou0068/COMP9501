
Simple distance metrics are the most commonly used metrics in KNN classification, for their calculations are relatively simple. We show that among three Minkowski distances, Euclidean and Manhattan distances perform well, while Chebyshev distance classifies much worse than them. Cosine distance outperforms three Minkowski distances for a better classification performance and it takes fewer computational resources.


Supervised metric learning methods obviously belongs to supervised learning algorithms.  They need to take training samples with labels as input and learn a distance matrix which will project data samples to a new space.  And they made samples in the same class relatively close while samples in different classes are far away from each other . We take several experiments based on LMNN, NCA, LFDA and MLKR algorithms to process the original data, and then use KNN to classify them.  Our experiments result shows that most of these methods can achieve a good accuracy(over 87\%) if we take appropriate parameters. Among these methods, LFDA has the best performance and the classification accuracy can reach 92\%. For every specific method, the selection of  K and feature dimension will also affect the result a lot. Usually a moderate value for K will get a better result, for it is enough to learn from neighbors and avoid noise samples at the same time. And reducing the sample dimension can significantly improve the classification efficiency, but it may also lead to the loss of accuracy, we need to find a balance in getting a satisfactory result.



Weakly supervised metric learning methods do not require the label information. They learn metrics based on tuples or triplets of similar/dissimilar data points. We combine several weakly supervised metric learning methods, such as ITML, LSML, SCML and RCA, with KNN classification and carry out a lot of experiments. Our experiments show that since weakly supervised metric learning methods usually involve sampling and take less information from the data-set, most methods cannot perform well on the original AwA2 data-set. To address this problem, by using PCA to reduce the data samples into 64-dimensions, we show that most methods can achieve a good performance similar to that of the original Euclidean metric. Furthermore, with more samples, the classification performance shows an increasing trend, indicating that the metric learning method indeed helps. However, there still exists a trade-off between the high computational cost during training and the limited improved effect brought by metric learning.